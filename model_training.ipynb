{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3638,
     "status": "ok",
     "timestamp": 1742309604553,
     "user": {
      "displayName": "Arion Research",
      "userId": "04722231138222012289"
     },
     "user_tz": -330
    },
    "id": "-AcHU7oStRN6",
    "outputId": "2a5a8a24-a89e-4345-bb7d-ff7c206438e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 1.26.4\n",
      "Uninstalling numpy-1.26.4:\n",
      "  Would remove:\n",
      "    /usr/local/bin/f2py\n",
      "    /usr/local/lib/python3.11/dist-packages/numpy-1.26.4.dist-info/*\n",
      "    /usr/local/lib/python3.11/dist-packages/numpy.libs/libgfortran-040039e1.so.5.0.0\n",
      "    /usr/local/lib/python3.11/dist-packages/numpy.libs/libopenblas64_p-r0-0cf96a72.3.23.dev.so\n",
      "    /usr/local/lib/python3.11/dist-packages/numpy.libs/libquadmath-96973f99.so.0.0.0\n",
      "    /usr/local/lib/python3.11/dist-packages/numpy/*\n",
      "Proceed (Y/n)? y\n",
      "  Successfully uninstalled numpy-1.26.4\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3934,
     "status": "ok",
     "timestamp": 1742309675177,
     "user": {
      "displayName": "Arion Research",
      "userId": "04722231138222012289"
     },
     "user_tz": -330
    },
    "id": "kLBa7-zHllgw",
    "outputId": "56e8f88e-7a40-4255-f2a4-5a3bb2f808cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "%pip install \"numpy<2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 165266,
     "status": "ok",
     "timestamp": 1742309575426,
     "user": {
      "displayName": "Arion Research",
      "userId": "04722231138222012289"
     },
     "user_tz": -330
    },
    "id": "L9IZpc1FVSAD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.0.1 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1) (4.11.0)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1) (1.13.3)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (72.1.0)\n",
      "Requirement already satisfied: wheel in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.43.0)\n",
      "Requirement already satisfied: cmake in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from triton==2.0.0->torch==2.0.1) (3.31.6)\n",
      "Requirement already satisfied: lit in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from triton==2.0.0->torch==2.0.1) (18.1.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from jinja2->torch==2.0.1) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from sympy->torch==2.0.1) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torchvision==0.15.2 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (0.15.2)\n",
      "Requirement already satisfied: numpy in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torchvision==0.15.2) (1.26.4)\n",
      "Requirement already satisfied: requests in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torchvision==0.15.2) (2.32.3)\n",
      "Requirement already satisfied: torch==2.0.1 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torchvision==0.15.2) (2.0.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torchvision==0.15.2) (11.1.0)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchvision==0.15.2) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchvision==0.15.2) (4.11.0)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchvision==0.15.2) (1.13.3)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchvision==0.15.2) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchvision==0.15.2) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchvision==0.15.2) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchvision==0.15.2) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchvision==0.15.2) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchvision==0.15.2) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchvision==0.15.2) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchvision==0.15.2) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchvision==0.15.2) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchvision==0.15.2) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchvision==0.15.2) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchvision==0.15.2) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchvision==0.15.2) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchvision==0.15.2) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchvision==0.15.2) (72.1.0)\n",
      "Requirement already satisfied: wheel in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchvision==0.15.2) (0.43.0)\n",
      "Requirement already satisfied: cmake in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from triton==2.0.0->torch==2.0.1->torchvision==0.15.2) (3.31.6)\n",
      "Requirement already satisfied: lit in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from triton==2.0.0->torch==2.0.1->torchvision==0.15.2) (18.1.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from requests->torchvision==0.15.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from requests->torchvision==0.15.2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from requests->torchvision==0.15.2) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from requests->torchvision==0.15.2) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from jinja2->torch==2.0.1->torchvision==0.15.2) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from sympy->torch==2.0.1->torchvision==0.15.2) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torchaudio==2.0.2 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (2.0.2)\n",
      "Requirement already satisfied: torch==2.0.1 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torchaudio==2.0.2) (2.0.1)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchaudio==2.0.2) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchaudio==2.0.2) (4.11.0)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchaudio==2.0.2) (1.13.3)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchaudio==2.0.2) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchaudio==2.0.2) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchaudio==2.0.2) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchaudio==2.0.2) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchaudio==2.0.2) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchaudio==2.0.2) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchaudio==2.0.2) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchaudio==2.0.2) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchaudio==2.0.2) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchaudio==2.0.2) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchaudio==2.0.2) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchaudio==2.0.2) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchaudio==2.0.2) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchaudio==2.0.2) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchaudio==2.0.2) (72.1.0)\n",
      "Requirement already satisfied: wheel in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchaudio==2.0.2) (0.43.0)\n",
      "Requirement already satisfied: cmake in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from triton==2.0.0->torch==2.0.1->torchaudio==2.0.2) (3.31.6)\n",
      "Requirement already satisfied: lit in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from triton==2.0.0->torch==2.0.1->torchaudio==2.0.2) (18.1.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from jinja2->torch==2.0.1->torchaudio==2.0.2) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from sympy->torch==2.0.1->torchaudio==2.0.2) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torchtext==0.15.2 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (0.15.2)\n",
      "Requirement already satisfied: tqdm in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torchtext==0.15.2) (4.66.5)\n",
      "Requirement already satisfied: requests in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torchtext==0.15.2) (2.32.3)\n",
      "Requirement already satisfied: torch==2.0.1 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torchtext==0.15.2) (2.0.1)\n",
      "Requirement already satisfied: numpy in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torchtext==0.15.2) (1.26.4)\n",
      "Requirement already satisfied: torchdata==0.6.1 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torchtext==0.15.2) (0.6.1)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchtext==0.15.2) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchtext==0.15.2) (4.11.0)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchtext==0.15.2) (1.13.3)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchtext==0.15.2) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchtext==0.15.2) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchtext==0.15.2) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchtext==0.15.2) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchtext==0.15.2) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchtext==0.15.2) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchtext==0.15.2) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchtext==0.15.2) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchtext==0.15.2) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchtext==0.15.2) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchtext==0.15.2) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchtext==0.15.2) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchtext==0.15.2) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch==2.0.1->torchtext==0.15.2) (2.0.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torchdata==0.6.1->torchtext==0.15.2) (2.2.2)\n",
      "Requirement already satisfied: setuptools in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchtext==0.15.2) (72.1.0)\n",
      "Requirement already satisfied: wheel in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchtext==0.15.2) (0.43.0)\n",
      "Requirement already satisfied: cmake in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from triton==2.0.0->torch==2.0.1->torchtext==0.15.2) (3.31.6)\n",
      "Requirement already satisfied: lit in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from triton==2.0.0->torch==2.0.1->torchtext==0.15.2) (18.1.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from requests->torchtext==0.15.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from requests->torchtext==0.15.2) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from requests->torchtext==0.15.2) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from jinja2->torch==2.0.1->torchtext==0.15.2) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from sympy->torch==2.0.1->torchtext==0.15.2) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: datasets==2.15.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (2.15.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from datasets==2.15.0) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from datasets==2.15.0) (19.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from datasets==2.15.0) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from datasets==2.15.0) (0.3.7)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from datasets==2.15.0) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from datasets==2.15.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from datasets==2.15.0) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from datasets==2.15.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from datasets==2.15.0) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.15.0) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from datasets==2.15.0) (3.11.14)\n",
      "Requirement already satisfied: huggingface-hub>=0.18.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from datasets==2.15.0) (0.29.3)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from datasets==2.15.0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from datasets==2.15.0) (6.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from aiohttp->datasets==2.15.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from aiohttp->datasets==2.15.0) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from aiohttp->datasets==2.15.0) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from aiohttp->datasets==2.15.0) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from aiohttp->datasets==2.15.0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from aiohttp->datasets==2.15.0) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from aiohttp->datasets==2.15.0) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from aiohttp->datasets==2.15.0) (1.18.3)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from huggingface-hub>=0.18.0->datasets==2.15.0) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from huggingface-hub>=0.18.0->datasets==2.15.0) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.15.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.15.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.15.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.15.0) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from pandas->datasets==2.15.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from pandas->datasets==2.15.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from pandas->datasets==2.15.0) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.15.0) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting tokenizers==0.21.1\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from tokenizers==0.21.1) (0.29.3)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.1) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.1) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.1) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.1) (6.0.1)\n",
      "Requirement already satisfied: requests in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.1) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.1) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.1) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.1) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.1) (2024.8.30)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers\n",
      "Successfully installed tokenizers-0.21.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torchmetrics==1.0.3 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (1.0.3)\n",
      "Requirement already satisfied: numpy>1.20.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torchmetrics==1.0.3) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.8.1 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torchmetrics==1.0.3) (2.0.1)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torchmetrics==1.0.3) (24.1)\n",
      "Requirement already satisfied: lightning-utilities>=0.7.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torchmetrics==1.0.3) (0.14.1)\n",
      "Requirement already satisfied: setuptools in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from lightning-utilities>=0.7.0->torchmetrics==1.0.3) (72.1.0)\n",
      "Requirement already satisfied: typing_extensions in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from lightning-utilities>=0.7.0->torchmetrics==1.0.3) (4.11.0)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch>=1.8.1->torchmetrics==1.0.3) (3.18.0)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch>=1.8.1->torchmetrics==1.0.3) (1.13.3)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch>=1.8.1->torchmetrics==1.0.3) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch>=1.8.1->torchmetrics==1.0.3) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch>=1.8.1->torchmetrics==1.0.3) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch>=1.8.1->torchmetrics==1.0.3) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch>=1.8.1->torchmetrics==1.0.3) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch>=1.8.1->torchmetrics==1.0.3) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch>=1.8.1->torchmetrics==1.0.3) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch>=1.8.1->torchmetrics==1.0.3) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch>=1.8.1->torchmetrics==1.0.3) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch>=1.8.1->torchmetrics==1.0.3) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch>=1.8.1->torchmetrics==1.0.3) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch>=1.8.1->torchmetrics==1.0.3) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch>=1.8.1->torchmetrics==1.0.3) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from torch>=1.8.1->torchmetrics==1.0.3) (2.0.0)\n",
      "Requirement already satisfied: wheel in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.8.1->torchmetrics==1.0.3) (0.43.0)\n",
      "Requirement already satisfied: cmake in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics==1.0.3) (3.31.6)\n",
      "Requirement already satisfied: lit in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics==1.0.3) (18.1.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from jinja2->torch>=1.8.1->torchmetrics==1.0.3) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from sympy->torch>=1.8.1->torchmetrics==1.0.3) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorboard==2.18.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (2.18.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from tensorboard==2.18.0) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from tensorboard==2.18.0) (1.71.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from tensorboard==2.18.0) (3.7)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from tensorboard==2.18.0) (1.26.4)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from tensorboard==2.18.0) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from tensorboard==2.18.0) (4.25.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from tensorboard==2.18.0) (72.1.0)\n",
      "Requirement already satisfied: six>1.9 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from tensorboard==2.18.0) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from tensorboard==2.18.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from tensorboard==2.18.0) (3.1.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard==2.18.0) (7.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard==2.18.0) (2.1.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard==2.18.0) (3.17.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: altair==5.1.1 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (5.1.1)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from altair==5.1.1) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from altair==5.1.1) (4.19.2)\n",
      "Requirement already satisfied: numpy in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from altair==5.1.1) (1.26.4)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from altair==5.1.1) (24.1)\n",
      "Requirement already satisfied: pandas>=0.25 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from altair==5.1.1) (2.2.3)\n",
      "Requirement already satisfied: toolz in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from altair==5.1.1) (1.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from altair==5.1.1) (4.11.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from jsonschema>=3.0->altair==5.1.1) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from jsonschema>=3.0->altair==5.1.1) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from jsonschema>=3.0->altair==5.1.1) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from jsonschema>=3.0->altair==5.1.1) (0.10.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from pandas>=0.25->altair==5.1.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from pandas>=0.25->altair==5.1.1) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from pandas>=0.25->altair==5.1.1) (2025.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from jinja2->altair==5.1.1) (2.1.3)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas>=0.25->altair==5.1.1) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: wandb==0.15.9 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (0.15.9)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from wandb==0.15.9) (8.1.8)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from wandb==0.15.9) (3.1.44)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from wandb==0.15.9) (2.32.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from wandb==0.15.9) (5.9.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from wandb==0.15.9) (2.23.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from wandb==0.15.9) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from wandb==0.15.9) (6.0.1)\n",
      "Requirement already satisfied: pathtools in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from wandb==0.15.9) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from wandb==0.15.9) (1.3.5)\n",
      "Requirement already satisfied: setuptools in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from wandb==0.15.9) (72.1.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from wandb==0.15.9) (1.4.4)\n",
      "Requirement already satisfied: typing-extensions in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from wandb==0.15.9) (4.11.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from wandb==0.15.9) (4.25.6)\n",
      "Requirement already satisfied: six>=1.4.0 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb==0.15.9) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb==0.15.9) (4.0.12)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb==0.15.9) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb==0.15.9) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb==0.15.9) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb==0.15.9) (2024.8.30)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb==0.15.9) (5.0.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy==1.26.4 in /root/miniconda3/envs/workenv/lib/python3.9/site-packages (1.26.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Use Python 3.9 environment (ensure you're running Python 3.9)\n",
    "%pip install torch==2.0.1\n",
    "%pip install torchvision==0.15.2\n",
    "%pip install torchaudio==2.0.2\n",
    "%pip install torchtext==0.15.2\n",
    "%pip install datasets==2.15.0\n",
    "%pip install tokenizers==0.21.1  # Fixing tokenizers version issue\n",
    "%pip install torchmetrics==1.0.3\n",
    "%pip install tensorboard==2.18.0  # Fixing TensorBoard version issue\n",
    "%pip install altair==5.1.1\n",
    "%pip install wandb==0.15.9\n",
    "%pip install numpy==1.26.4  # Ensure NumPy is installed properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3166,
     "status": "ok",
     "timestamp": 1742309681097,
     "user": {
      "displayName": "Arion Research",
      "userId": "04722231138222012289"
     },
     "user_tz": -330
    },
    "id": "4sxqMp57szOP",
    "outputId": "c99e9fab-99b9-47b7-8325-a94787248d6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy\n",
    "print(torch.tensor([1, 2, 3]).numpy())  # Should not give an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1742309681631,
     "user": {
      "displayName": "Arion Research",
      "userId": "04722231138222012289"
     },
     "user_tz": -330
    },
    "id": "8pxAMKFBlfqo",
    "outputId": "92fe4fd4-c9c0-4893-9795-7155e73962d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.26.4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1742309687483,
     "user": {
      "displayName": "Arion Research",
      "userId": "04722231138222012289"
     },
     "user_tz": -330
    },
    "id": "M3uFSw9MJOhK"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def get_config():\n",
    "    return {\n",
    "        \"batch_size\": 8,\n",
    "        \"num_epochs\": 50,\n",
    "        \"lr\": 10**-4,\n",
    "        \"seq_len\": 500,\n",
    "        \"d_model\": 512,\n",
    "        \"datasource\": 'opus_books',\n",
    "        \"lang_src\": \"en\",\n",
    "        \"lang_tgt\": \"fr\",\n",
    "        \"model_folder\": \"weights\",\n",
    "        \"model_basename\": \"tmodel_\",\n",
    "        \"preload\": \"latest\",\n",
    "        \"tokenizer_file\": \"tokenizer_{0}.json\",\n",
    "        \"experiment_name\": \"runs/tmodel\"\n",
    "    }\n",
    "\n",
    "def get_weights_file_path(config, epoch: str):\n",
    "    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
    "    model_filename = f\"{config['model_basename']}{epoch}.pt\"\n",
    "    return str(Path('.') / model_folder / model_filename)\n",
    "\n",
    "# Find the latest weights file in the weights folder\n",
    "def latest_weights_file_path(config):\n",
    "    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
    "    model_filename = f\"{config['model_basename']}*\"\n",
    "    weights_files = list(Path(model_folder).glob(model_filename))\n",
    "    if len(weights_files) == 0:\n",
    "        return None\n",
    "    weights_files.sort()\n",
    "    return str(weights_files[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1742309688423,
     "user": {
      "displayName": "Arion Research",
      "userId": "04722231138222012289"
     },
     "user_tz": -330
    },
    "id": "BsHmPE1hVCXF"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class BilingualDataset(Dataset):\n",
    "\n",
    "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "\n",
    "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_target_pair = self.ds[idx]\n",
    "        src_text = src_target_pair['translation'][self.src_lang]\n",
    "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
    "\n",
    "        # Transform the text into tokens\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "        # Add sos, eos and padding to each sentence\n",
    "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2  # We will add <s> and </s>\n",
    "        # We will only add <s>, and </s> only on the label\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
    "\n",
    "        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError(\"Sentence is too long\")\n",
    "\n",
    "        # Add <s> and </s> token\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Add only <s> token\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Add only </s> token\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Double check the size of the tensors to make sure they are all seq_len long\n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,  # (seq_len)\n",
    "            \"decoder_input\": decoder_input,  # (seq_len)\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, seq_len) & (1, seq_len, seq_len),\n",
    "            \"label\": label,  # (seq_len)\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text,\n",
    "        }\n",
    "\n",
    "def causal_mask(size):\n",
    "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1742309689484,
     "user": {
      "displayName": "Arion Research",
      "userId": "04722231138222012289"
     },
     "user_tz": -330
    },
    "id": "TC-bE6z5VGOS"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, eps:float=10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(features)) # alpha is a learnable parameter\n",
    "        self.bias = nn.Parameter(torch.zeros(features)) # bias is a learnable parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, hidden_size)\n",
    "         # Keep the dimension for broadcasting\n",
    "        mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # Keep the dimension for broadcasting\n",
    "        std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # eps is to prevent dividing by zero or when std is very small\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
    "\n",
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
    "\n",
    "class InputEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len) --> (batch, seq_len, d_model)\n",
    "        # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a matrix of shape (seq_len, d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        # Create a vector of shape (seq_len)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "        # Create a vector of shape (d_model)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
    "        # Add a batch dimension to the positional encoding\n",
    "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "        # Register the positional encoding as a buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class ResidualConnection(nn.Module):\n",
    "\n",
    "        def __init__(self, features: int, dropout: float) -> None:\n",
    "            super().__init__()\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.norm = LayerNormalization(features)\n",
    "\n",
    "        def forward(self, x, sublayer):\n",
    "            return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Embedding vector size\n",
    "        self.h = h # Number of heads\n",
    "        # Make sure d_model is divisible by h\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "\n",
    "        self.d_k = d_model // h # Dimension of vector seen by each head\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        return self.w_o(x)\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class ProjectionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x) -> None:\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
    "        return self.proj(x)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        # (batch, seq_len, d_model)\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "\n",
    "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        # (batch, seq_len, d_model)\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "\n",
    "    def project(self, x):\n",
    "        # (batch, seq_len, vocab_size)\n",
    "        return self.projection_layer(x)\n",
    "\n",
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n",
    "    # Create the embedding layers\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "    # Create the positional encoding layers\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
    "\n",
    "    # Create the encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    # Create the decoder blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "\n",
    "    # Create the encoder and decoder\n",
    "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
    "\n",
    "    # Create the projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "\n",
    "    # Create the transformer\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "\n",
    "    # Initialize the parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d4c72c83250248a181ee990c24600eca",
      "0e815bd727b7497f81ae2295133c4e46",
      "c360499999c14412b4f4034d339da90e",
      "df95cdd2ba6342e69eb0fa751e95428e",
      "bfbc661de29343f89b13ccaaf119ff32",
      "af66a79b762344cb8f3aed1fb32d61ea",
      "d769fe7ac4d94869acfe809dd0f8a7c0",
      "f2eede091e1747c9870494e6e338ee54",
      "92e7a01811e940c388d69aec398d836f",
      "a60d141b29ec45dc8025c92d2991c0a7",
      "c6387916e7ac4810a770f58d774d7abb",
      "dea5b7677e6248a0bfa8350086ef072e",
      "029b589847fc40e185dd5be4090ebe55",
      "ab9e0bfe176641ebb1ba7b2dedc9484a",
      "7f41c6b6c92e4b39af0217d3e8a82057",
      "a6e45638b7444eb99b584147171985e6",
      "7f1bbd51d0fd4fabb0a8b4da01badce1",
      "96f57b87f09143618274d311b06b4a0a",
      "02e48a31ac7b4d29a50f491b2b31ea44",
      "f2e53a2f2e7a4b288b7a00f814e8d559",
      "a34586a33b7c48a88667e55f399d4f56",
      "dc1db77b38af42879636c08865e9d2a0",
      "edeb661a95084caf9aba4e4f24aaf302",
      "e05889af986c4fd79b3fa22cac87192b",
      "ef3b575c458d4c05b26c96538713c25d",
      "126dd435d6b044e7909dc97c1f536cc6",
      "d7cc4632320b40d4b3d188d486095709",
      "b6b86ec690d748d8ad7d5d99cf553527",
      "2abf772dcdc042aabb2bf5b25b46ade6",
      "311f821f5ff4474897e788dcb8b9a410",
      "25938b5e07a642eea1c2435cf0ea8080",
      "3df8daa0bdc14633b21a985555fbe5ed",
      "ccc96502294949ccb4e3779a42b0ea26",
      "14d7f6a81ee74ab1a9815ccf4bb1bc74",
      "11d2382851074f8489a61e4871ec711d",
      "03e702becc2d46f28068f230f1670923",
      "f557bbe2d0d74c9db667d7057007a8b7",
      "1fb272bcdc3f4962b529a93db8a0eb40",
      "942bc81cd7874722bcf7c878f5edef8a",
      "c65882c78fd245998b84f37ff2ee12b1",
      "3d03a0dc7f41415bb70c2dc4c32f2edc",
      "2e5d375ba6ea46e8a997c4fd617730cd",
      "0870ff2092df4e4990a86007ac1ed9b0",
      "f21e116b03694620ad28b08e742a2b72",
      "5a566846f6b04c01a8925bcf387e893b",
      "183c1350f5e74202b856daf48238ab9a",
      "715f7d49656a4392b5c00766166f0125",
      "a3bd82c96af0409eb7860b39c956b9db",
      "194bf931fd924396a8a7411c40dcd894",
      "8c0670ce8bcc470d967add0fc1bf8f44",
      "e3ecddfae3bf48b9a3506eb553cc927d",
      "031e60d11dc749d7af95a349a81b72fa",
      "7b9a7eee6fe2479e9cbca1a79ae6643e",
      "ae9442fe6f354470b59b59e4473e523f",
      "88ebeb75e06447988192f2c99c0d216a"
     ]
    },
    "executionInfo": {
     "elapsed": 6485646,
     "status": "error",
     "timestamp": 1742316176014,
     "user": {
      "displayName": "Arion Research",
      "userId": "04722231138222012289"
     },
     "user_tz": -330
    },
    "id": "UnoxyGZUVgxc",
    "outputId": "1d387756-a3ed-49d0-b0a3-e4b7f45a6290"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Device name: NVIDIA GeForce RTX 4090\n",
      "Device memory: 23.64971923828125 GB\n",
      "Max length of source sentence: 471\n",
      "Max length of target sentence: 482\n",
      "No model to preload, starting from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 00: 100%|██████████| 14297/14297 [25:32<00:00,  9.33it/s, loss=4.784]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: – Pas la première parole ; j’ai quitté Madame à six heures, joyeuse et contente.\n",
      "    TARGET: \"Not a word; I left the Signora at six o'clock, happy and content.\"\n",
      " PREDICTED: \" This the ; I shall have been a six or , or , and we shall have been .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: They owed Lestiboudois for so many days. Then the child grew cold and asked for her mother.\n",
      "    TARGET: Puis l’enfant avait froid et demandait sa mère.\n",
      " PREDICTED: Ils étaient plus tard , si la pauvre fille était encore , et le jeune garçon lui demanda .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 01: 100%|██████████| 14297/14297 [25:33<00:00,  9.32it/s, loss=4.147]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: By God it is!\"\n",
      "    TARGET: Nom de Dieu de nom de Dieu!\n",
      " PREDICTED: À Dieu ! c ' est toi !\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Ha!\n",
      "    TARGET: – Ah !\n",
      " PREDICTED: -- Ah !\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 02: 100%|██████████| 14297/14297 [25:32<00:00,  9.33it/s, loss=3.551]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: But Camille insisted on these Sunday outings, which gave him the satisfaction of showing off his wife. When he met a colleague, particularly one of his chiefs, he felt quite proud to exchange bows with him, in the company of Madame.\n",
      "    TARGET: Mais Camille tenait bon; il aimait à montrer sa femme; lorsqu'il rencontrait un de ses collègues, un de ses chefs surtout, il était tout fier d'échanger un salut avec lui, en compagnie de madame.\n",
      " PREDICTED: Mais Camille insista sur ces , qui lui donna la satisfaction de faire suivre sa femme ; et , comme il vit un voleur , il sentait tout fier à lui , à l ' avance , à la compagnie de madame .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: He, on the contrary is yearning to take you by the hand, and talk to you.\n",
      "    TARGET: Lui, au contraire, est désireux de vous serrer la main et de causer avec vous.\n",
      " PREDICTED: Il est , au contraire , vous avez raison de vous mettre en main par la main , et vous parlez .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 03: 100%|██████████| 14297/14297 [25:32<00:00,  9.33it/s, loss=3.781]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Then she went on to tell me how she very luckily fell into a good family, where, behaving herself well, and her mistress dying, her master married her, by whom she had my husband and his sister, and that by her diligence and good management after her husband's death, she had improved the plantations to such a degree as they then were, so that most of the estate was of her getting, not her husband's, for she had been a widow upwards of sixteen years.\n",
      "    TARGET: Puis elle continua à me raconter comment elle était tombée entre les mains d'une bonne famille, où, par sa bonne conduite, sa maîtresse étant morte, son maître l'avait épousée, et c'est de lui qu'elle avait eu mon mari et ma soeur; et comment, par sa diligence et son bon gouvernement, après la mort de son mari, elle avait amélioré les plantations à un point qu'elles n'avaient pas atteint jusque-là, si bien que la plus grande partie des terres avaient été mises en culture par elle, non par son mari; car elle était veuve depuis plus de seize ans.\n",
      " PREDICTED: Puis elle continua à me dire comment elle était très bonne , où elle se trouvait bien , elle se mit à se rendre compte , et sa maîtresse se , par qui elle avait mon mari et sa soeur , et que par sa bonne conduite et la bonne administration de son mari , elle avait la mort de telle sorte qu ' ils étaient si bien de son mari , qu ' elle avait été de sa fortune , qu ' elle avait été de sa maîtresse , et qu ' elle avait été digne de son mari .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Oh! what a displeasing hunchback's face!\"\n",
      "    TARGET: – Oh ! la déplaisante face de bossu !\n",
      " PREDICTED: -- Oh ! quel coup de bossu !\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 04: 100%|██████████| 14297/14297 [25:34<00:00,  9.32it/s, loss=3.373]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"But, first of all, it was necessary that I should make arrangements for the wife and the son, of whose existence you and my other friends were ignorant.\n",
      "    TARGET: «Mais il fallait avant tout prendre des mesures au sujet de ma femme et de mon fils dont vous et mes autres amis ignoriez l'existence.\n",
      " PREDICTED: -- Mais , d ' abord , il me fallait que je pour la femme et le fils de l ' existence dont vous et mes amis ont été .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Il lui croyait beaucoup d’esprit et il était furieux de ce qu’elle s’obstinait évidemment à ne pas ouvrir un avis.\n",
      "    TARGET: he credited her with great intelligence, and was furious at her evident refusal to offer him any advice.\n",
      " PREDICTED: He thought he felt great mind , and he was a fury of this which she was to herself , not a advice .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 05: 100%|██████████| 14297/14297 [25:33<00:00,  9.32it/s, loss=3.045]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"But if you saw him so seldom and wrote so seldom, how did he know enough about your affairs to be able to help you, as you say that he has done?\"\n",
      "    TARGET: – Mais si vous l’avez vu et lui avez écrit si rarement, comment en savait-il assez sur vos affaires pour vous aider ? »\n",
      " PREDICTED: -- Mais si vous l ' avez vu tant de fois , et il a écrit si rarement , comment a - t - il su que vous alliez vous aider , comme vous dites qu ' il a fait ?\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Do you know what I did that evening, after the Opera Comique?\"\n",
      "    TARGET: Savez-vous ce que j'ai fait le soir de l'Opéra-Comique?\n",
      " PREDICTED: Savez - vous ce que j ' ai fait ce soir , après l ' Opéra - Comique ?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 06: 100%|██████████| 14297/14297 [25:34<00:00,  9.32it/s, loss=3.088]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"To-morrow work begins again at the Voreux.\n",
      "    TARGET: —C'est demain que le travail reprend au Voreux.\n",
      " PREDICTED: — A demain , le Voreux commence a nouveau au Voreux .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: A dim line of ancestors, in every variety of dress, from the Elizabethan knight to the buck of the Regency, stared down upon us and daunted us by their silent company.\n",
      "    TARGET: Toute une rangée d’ancêtres, dans une bizarre variété de costumes, depuis le chevalier élisabéthain jusqu’au dandy de la Régence, plongeaient leurs regards fixes sur nous et nous impressionnaient par leur présence silencieuse.\n",
      " PREDICTED: Une ligne de ancêtres , dans toutes les de la , du chevalier du , nous regardait et nous par leurs cris de compagnie .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 07: 100%|██████████| 14297/14297 [25:35<00:00,  9.31it/s, loss=2.804]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: The river still measured from sixty to seventy feet in breadth, and its bed from five to six feet in depth.\n",
      "    TARGET: La rivière mesurait encore soixante à soixante-dix pieds de large, et son lit cinq à six pieds de profondeur.\n",
      " PREDICTED: La Tamise mesurait encore soixante - dix pieds de largeur , et son lit de cinq à six pieds de profondeur .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: The two men were separated, while Chaval, who was quite calm, only repeated:\n",
      "    TARGET: On se précipita entre les deux hommes, tandis que Chaval, tres calme, répétait:\n",
      " PREDICTED: Les deux hommes se séparaient , tandis que Chaval , tres calme , répétait :\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 08: 100%|██████████| 14297/14297 [25:34<00:00,  9.32it/s, loss=2.975]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: There was in the two ways in which \"Master Jacques\" was pronounced on the one hand, and the \"master\" by preeminence on the other, the difference between monseigneur and monsieur, between ~domine~ and ~domne~.\n",
      "    TARGET: Il y avait dans les deux manières dont fut prononcé d’une part ce maître Jacques, de l’autre ce maître par excellence, la différence du monseigneur au monsieur, du domine au domne.\n",
      " PREDICTED: Il y avait dans les deux façons que maître Jacques Jacques , la main , et le maître de , l ’ autre , la différence entre Monseigneur et M . le , entre et .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: I want you to tell me whether you love me.\"\n",
      "    TARGET: Je veux que vous me disiez si vous m’aimez.\n",
      " PREDICTED: Je veux que vous me dise si vous m ' aimez .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 09: 100%|██████████| 14297/14297 [25:34<00:00,  9.32it/s, loss=3.116]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Ce soir-là Fabrice entendait fort distinctement un grand nombre d’hommes passer sur le pont en fer, dit le pont de l’esclave, parce que jadis un esclave dalmate avait réussi à se sauver, en précipitant le gardien du pont dans la cour.\n",
      "    TARGET: That evening Fabrizio could hear quite distinctly a considerable number of men cross the iron bridge, known as the Slave's Bridge, because once a Dalmatian slave had succeeded in escaping, by throwing the guardian of the bridge down into the court below.\n",
      " PREDICTED: This evening Fabrizio heard quite plainly a number of men who were passing on the bridge , like iron , said the bridge of the Raversi , because a actor had had managed to save himself , in the of the bridge in court .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Everything contributed to my joy, from the paltry pleasure of awaiting the Thursday holiday to the great discovery we had just made and the fine piece of luck befalling us.\n",
      "    TARGET: Tout se mêlait pour contribuer à ma joie, depuis le faible plaisir que donnait l’attente du jeudi jusqu’à la très grande découverte que nous venions de faire, jusqu’à la très grande chance qui nous était échue.\n",
      " PREDICTED: Tout me à ma joie , par le plaisir que nous avions de nous attendre le jeudi de la grande découverte que nous venions de faire , et la belle nouvelle de nous marier .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 10: 100%|██████████| 14297/14297 [25:31<00:00,  9.33it/s, loss=2.712]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: I found the black tor upon which I had seen the solitary watcher, and from its craggy summit I looked out myself across the melancholy downs.\n",
      "    TARGET: Je retrouvais le pic noir sur lequel j’avais vu le guetteur solitaire, je l’escaladai et de son sommet tourmenté je contemplai la mélancolie du paysage.\n",
      " PREDICTED: Je trouvai le pic noir sur lequel j ' avais vu le solitaire , et de sa crête je me trouvai à travers les dunes sombres .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Along the narrow walls of this passageway, I saw only brilliant streaks, hard lines, fiery furrows, all scrawled by our speeding electric light.\n",
      "    TARGET: Sur les murailles étroites du passage, je ne voyais plus que des raies éclatantes, des lignes droites, des sillons de feu tracés par la vitesse sous l'éclat de l'électricité.\n",
      " PREDICTED: Sur les étroites parois de ce passage , je ne vis que de légers raies , de véritables traînées de feu , de sillons , tout par la lueur électrique .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 11: 100%|██████████| 14297/14297 [25:31<00:00,  9.34it/s, loss=2.945]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Therefore the carpenters lost not a moment.\n",
      "    TARGET: Aussi les charpentiers ne perdirent-ils pas un moment.\n",
      " PREDICTED: Donc les charpentiers ne se perdaient pas un instant .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: CHAPTER XIV\n",
      "    TARGET: CHAPITRE XIV\n",
      " PREDICTED: XIV\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 12: 100%|██████████| 14297/14297 [25:32<00:00,  9.33it/s, loss=2.349]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: I would gladly have sent my husband away to Caroline with all our goods, and have come after myself, but this was impracticable; he would never stir without me, being himself perfectly unacquainted with the country, and with the methods of settling there or anywhere else.\n",
      "    TARGET: J'aurai aimé à envoyer mon mari en Caroline pour le suivre ensuite moi-même, mais c'était impraticable, parce qu'il ne voulait pas bouger sans moi, ne connaissant nullement le pays ni la manière de s'établir en lieu que ce fut.\n",
      " PREDICTED: J ’ aurais volontiers envoyé mon mari en Caroline avec toutes nos marchandises , et j ’ aurais suivi moi - même ; mais il était impraticable ; il ne se jamais sans que je pusse en avoir connaissance avec la campagne , et avec les moyens de s ’ établir ou ailleurs .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"I might have known it.\n",
      "    TARGET: J'aurais dû m'en douter.\n",
      " PREDICTED: – J ’ aurais pu le savoir .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 13: 100%|██████████| 14297/14297 [25:32<00:00,  9.33it/s, loss=2.303]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: And why does he sign himself 'him whom you knew as Jim Harrison?'\n",
      "    TARGET: Et pourquoi signe-t-il celui que vous connaissiez sous le nom de James Harrison?\n",
      " PREDICTED: Et pourquoi se fait - il signer de lui que vous connaissiez comme Jim ?\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'Was I not speaking the truth?' thought Julien; 'why does the love that I felt for that madwoman torment me still?'\n",
      "    TARGET: Que n’ai-je dit vrai ? pensait Julien, pourquoi l’amour que j’avais pour cette folle me tourmente-t-il encore ?\n",
      " PREDICTED: N ’ étais - je pas la vérité ? pensa Julien ; pourquoi me fait - il pour cette folle supplice que je me sentais encore ?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 14: 100%|██████████| 14297/14297 [25:33<00:00,  9.33it/s, loss=2.927]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Leave the window open on his side, Carter; there is no wind--good-bye, Dick.\"\n",
      "    TARGET: -- Laissez la fenêtre ouverte de son côté, Carter; il n'y a pas de vent. Adieu, Dick.\n",
      " PREDICTED: -- Laissons la fenêtre ouverte , Carter ; il n ' y a pas de vent , adieu , Dick , Dick .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Pleased with the preference of one, and offended by the neglect of the other, on the very beginning of our acquaintance, I have courted prepossession and ignorance, and driven reason away, where either were concerned. Till this moment I never knew myself.\"\n",
      "    TARGET: Flattée de la préférence de l’un, froissée du manque d’égards de l’autre, je me suis abandonnée des le début a mes préventions et j’ai jugé l’un et l’autre en dépit du bon sens.\n",
      " PREDICTED: avec la préférence d ' un et offensé par la négliger de l ' autre , au commencement de notre connaissance , j ' ai eu de l ' ignorance et de l ' ignorance , et de la raison où il s ' agissait ; au moment où je ne me suis jamais trompée .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 15: 100%|██████████| 14297/14297 [25:30<00:00,  9.34it/s, loss=2.455]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: '\"If I were not afraid of spoiling the finest bass voice I have ever heard, I should lock you up on bread and water for a fortnight, you scoundrel.\"\n",
      "    TARGET: – Si je ne craignais pas de gâter la plus belle voix de basse que j’aie jamais entendue, je te mettrais en prison au pain et à l’eau pour quinze jours, polisson.\n",
      " PREDICTED: Si je n ’ avais pas peur de gâter la plus belle voix que j ’ aie jamais entendue , je t ’ aurais ouvert sur le pain et l ’ eau pendant une quinzaine , drôle .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'That author is most immoral,' Julien said to Madame Valenod; 'in one of his Fables on Messire Jean Chouart, he has ventured to heap ridicule on all that is most venerable.\n",
      "    TARGET: – Cet auteur est bien immoral, dit Julien à Mme Valenod, certaine fable, sur messire Jean Chouart, ose déverser le ridicule sur ce qu’il y a de plus vénérable.\n",
      " PREDICTED: – Cet auteur est le plus immoral , dit Julien à Mme Valenod ; dans une de ses de messire Jean , il s ’ est osé bouillir sur tout ce qui est le plus vénérable .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 16: 100%|██████████| 14297/14297 [25:28<00:00,  9.35it/s, loss=2.371]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Sometimes Herbert accompanied them, but never Pencroft, who could not bear to look upon the prospect of the island now so utterly devastated.\n",
      "    TARGET: Quelquefois Harbert les accompagnait, jamais Pencroff, qui ne voulait pas voir sous son aspect nouveau l'île si profondément dévastée!\n",
      " PREDICTED: Parfois , Harbert les accompagnait , mais jamais Pencroff , qui ne pouvait tenir en place de la perspective de l ' île si mortellement dévastés .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Then she would knowand understand everything.\n",
      "    TARGET: Alors elle saurait et comprendrait tout!\n",
      " PREDICTED: Alors elle tout .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 17: 100%|██████████| 14297/14297 [25:28<00:00,  9.36it/s, loss=2.441]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Ceci fait, trouvant sans doute qu’il avait achevé sa tâche, il remit son metre et sa loupe dans sa poche.\n",
      "    TARGET: This done, he appeared to be satisfied, for he replaced his tape and his glass in his pocket.\n",
      " PREDICTED: All this means , not being the doubt that he had his task , he took his metre and his glass in his pocket .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Heaven forbid! _That_ would be the greatest misfortune of all!\n",
      "    TARGET: – Le ciel m’en préserve.\n",
      " PREDICTED: – Dieu me garde ! ce serait le plus grand malheur !\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 18: 100%|██████████| 14297/14297 [25:28<00:00,  9.35it/s, loss=2.479]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: And the engine-man's eyes went from the young girl to her companion, while he stepped back with a sudden, relinquishing movement.\n",
      "    TARGET: Et les yeux du machineur allerent de la jeune fille au camarade; tandis qu'il reculait d'un pas, avec un geste de brusque abandon.\n",
      " PREDICTED: Et les yeux de la machine , d ' apres son compagnon , sortaient de sa voisine , il s ' éloigna soudain , jouant des mouvements brusques .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: I wanted no arguments.\n",
      "    TARGET: Je ne voulais pas discuter.\n",
      " PREDICTED: Je ne voulus pas m ' en douter .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 19: 100%|██████████| 14297/14297 [25:29<00:00,  9.35it/s, loss=2.195]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Certain indigenous plants were discovered, and those fit for immediate use contributed to vary the vegetable stores of Granite House.\n",
      "    TARGET: Certaines plantes indigènes furent encore découvertes, et, si elles n'avaient pas une utilité immédiate, elles contribuèrent à varier les réserves végétales de Granite-House.\n",
      " PREDICTED: Certains plantes furent découvertes , et l ' avantage de naviguer immédiatement la matière végétale de Granite - House .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: We are not on duty, and we believed that not being on duty we were at liberty to dispose of our time as we pleased.\n",
      "    TARGET: Nous ne sommes pas de service, et nous avons cru que, n'étant pas de service, nous pouvions disposer de notre temps comme bon nous semblait.\n",
      " PREDICTED: Nous ne sommes pas en devoir , et nous sommes qu ’ il ne fallait pas avoir la liberté d ’ abandonner notre temps de départ .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 20: 100%|██████████| 14297/14297 [25:31<00:00,  9.34it/s, loss=2.182]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: D’Artagnan blushed up to the whites of his eyes.\n",
      "    TARGET: D'Artagnan rougit jusqu'au blanc des yeux.\n",
      " PREDICTED: D ' Artagnan rougit jusqu ' aux yeux .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Often his mistress's sincere admiration, and her transports of passion made him forget the fatuous theory that had kept him so restrained and almost ridiculous in the first moments of their intimacy.\n",
      "    TARGET: Souvent la sincère admiration et les transports de sa maîtresse lui faisaient oublier la vaine théorie qui l’avait rendu si compassé et presque si ridicule dans les premiers moments de cette liaison.\n",
      " PREDICTED: Souvent la sincère admiration de sa maîtresse , ses transports de passion lui faisaient oublier la théorie bête bête qui l ’ avait si mal gardée et presque ridicule aux premières moments de leur intimité .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 21: 100%|██████████| 14297/14297 [25:30<00:00,  9.34it/s, loss=2.332]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Ludovic rentra chargé d’un paquet.\n",
      "    TARGET: Lodovico returned, carrying a packet.\n",
      " PREDICTED: Lodovico returned to a .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: The camera was aimed at the scenery on the ocean floor, and in a few seconds we had a perfect negative.\n",
      "    TARGET: L'instrument fut braqué sur ces sites du fond océanique, et en quelques secondes, nous avions obtenu un négatif d'une extrême pureté.\n",
      " PREDICTED: Les furent disposées sur le fond de l ' Océan , et dans quelques instants nous eûmes un véritable négatif .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 22: 100%|██████████| 14297/14297 [25:30<00:00,  9.34it/s, loss=2.326]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Well, but Pierre.\n",
      "    TARGET: --Oui, mais Pierre?\n",
      " PREDICTED: -- Mais Pierre …\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Aramis received a ball which passed through his shoulder, and Mousqueton another ball which lodged in the fleshy part which prolongs the lower portion of the loins.\n",
      "    TARGET: Aramis reçut une balle qui lui traversa l'épaule, et Mousqueton une autre balle qui se logea dans les parties charnues qui prolongent le bas des reins.\n",
      " PREDICTED: Aramis reçut un bal qui lui passait par l ' épaule , et Mousqueton , qui se dans la partie charnues qui la partie inférieure des reins .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 23: 100%|██████████| 14297/14297 [25:31<00:00,  9.34it/s, loss=2.129]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: It was Fix, who, bowing, addressed Mr. Fogg: \"Were you not, like me, sir, a passenger by the Rangoon, which arrived yesterday?\"\n",
      "    TARGET: C'était l'inspecteur Fix, qui le salua et lui dit : « N'êtes-vous pas comme moi, monsieur, un des passagers du Rangoon, arrivé hier ?\n",
      " PREDICTED: C ' était Fix , qui , s ' adressant à Mr . Fogg : -- Vous n ' étiez pas comme moi , monsieur , un passager , arrivé à bord du Rangoon , qui est arrivé hier ?\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Si vous me donnez à Naples une place dans une loge à San Carlo et un cheval, je suis plus que satisfait ; ce ne sera jamais le plus ou moins de luxe qui nous donnera un rang à vous et à moi, c’est le plaisir que les gens d’esprit du pays pourront trouver peut-être à venir prendre une tasse de thé chez vous.\n",
      "    TARGET: If you give me, at Naples, a seat in a box at San Carlo and a horse, I am more than satisfied; it will never be the amount of luxury with which we live that will give you and me our position, it is the pleasure which the intelligent people of the place may perhaps find in coming to take a dish of tea with you.\"\n",
      " PREDICTED: If you give me to Naples a place in a box at San Giovanni and a horse , I am more than satisfied ; that is ever the most , or less luxury which give us a rank of you and me , it is the pleasure of the spirit of his mind to find perhaps also to come to make an extent for taking the tribunal of you .\"\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 24: 100%|██████████| 14297/14297 [25:29<00:00,  9.35it/s, loss=2.189]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'There, that's settled.'\n",
      "    TARGET: Allons, voilà qui est décidé.\n",
      " PREDICTED: – Là , c ’ est convenu .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: The portions were handed round; those who liked took a draught of the water, the mug being common to all.\n",
      "    TARGET: Les parts furent distribuées aux élèves, et celles qui avaient soif prirent un peu d'eau dans le gobelet qui servait à toutes.\n",
      " PREDICTED: Les portions étaient remplies , on le fit passer par un verre d ' eau , et le vase était commun , au point de tomber .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 25: 100%|██████████| 14297/14297 [25:29<00:00,  9.35it/s, loss=2.169]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: He does not care for that: when my time came to die, he would resign me, in all serenity and sanctity, to the God who gave me.\n",
      "    TARGET: Eh bien! après? peu lui importe à lui; quand l'heure de mourir sera venue, il me rendra avec un visage serein au Dieu qui m'aura donnée à lui.\n",
      " PREDICTED: Il n ' a garde de cela ; quand mon temps viendrait mourir , il me en sérénité , dans toute sainteté , au Dieu qui m ' a donné .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Leon walked up and down the room; it seemed strange to him to see this beautiful woman in her nankeen dress in the midst of all this poverty.\n",
      "    TARGET: Léon se promenait dans la chambre; il lui semblait étrange de voir cette belle dame en robe de nankin, tout au milieu de cette misère.\n",
      " PREDICTED: Léon se promena dans la salle , il lui semblait étrange de voir cette belle femme en robe de nankin , au milieu de toute cette pauvreté .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 26: 100%|██████████| 14297/14297 [25:29<00:00,  9.35it/s, loss=2.104]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: The distance, increased by detours and obstacles which could not be surmounted directly, was long.\n",
      "    TARGET: La route, accrue par des détours et des obstacles qui ne pouvaient être franchis directement, était longue.\n",
      " PREDICTED: La distance , par des détours et des obstacles qui ne pouvaient être directement , était longue .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: She turned red, then pale, and began to tremble like a culprit before the captain, who gazed at her with a smile of satisfaction and amazement.\n",
      "    TARGET: Elle devint rouge, puis pâle, et se mit à trembler comme une coupable devant le capitaine, qui la regardait avec un sourire de satisfaction et d’étonnement.\n",
      " PREDICTED: Elle pâlit , et commença à trembler comme un coupable devant le capitaine , qui la regardait avec un sourire d ' animation et de surprise .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 27: 100%|██████████| 14297/14297 [25:30<00:00,  9.34it/s, loss=2.079]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Diving easily, these reptiles can remain a good while underwater by closing the fleshy valves located at the external openings of their nasal passages.\n",
      "    TARGET: Ces reptiles, qui plongent facilement, peuvent se maintenir longtemps sous l'eau en fermant la soupape charnue située à l'orifice externe de leur canal nasal.\n",
      " PREDICTED: Ces reptiles , les reptiles peuvent rester longtemps dans les entrailles du rivage , à leur suite de ces cavernes du .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"If he has committed any crime, he has most fearfully expiated it, and in our eyes he is absolved.\"\n",
      "    TARGET: S'il a commis quelque faute, il l'a cruellement expiée, et, à nos yeux, il est absous.»\n",
      " PREDICTED: -- S ' il a commis quelque crime , il l ' a en moi très horriblement , et dans les yeux il est désarmé .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 28: 100%|██████████| 14297/14297 [25:37<00:00,  9.30it/s, loss=2.523]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: I experienced a strange feeling as the key grated in the lock, and the sound of his retreating step ceased to be heard.\n",
      "    TARGET: J'éprouvai une étrange sensation lorsque la clef cria dans la serrure et que je n'entendis plus le bruit de ses pas.\n",
      " PREDICTED: J ' éprouvais un étrange sentiment à la clef dans la serrure et le bruit de son pas qui cessa d ' être entendu .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: We left the house, and started on our return to Paris, talking over the new plan.\n",
      "    TARGET: Nous quittâmes la maison et reprîmes la route de Paris tout en causant de cette nouvelle résolution.\n",
      " PREDICTED: Nous quittâmes la maison , et nous partîmes en route pour Paris , en causant du nouveau plan .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 29: 100%|██████████| 14297/14297 [25:42<00:00,  9.27it/s, loss=1.885] \n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"The matter? the matter is that I have just broken a tooth!\" replied the sailor.\n",
      "    TARGET: -- Il y a... il y a... que je viens de me casser une dent! répondit le marin.\n",
      " PREDICTED: -- Eh ! l ' affaire est que je viens de briser une dent !» répondit le marin .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: He stood still and counted his pulse.\n",
      "    TARGET: Il s’arreta pour se tâter le pouls.\n",
      " PREDICTED: Il resta immobile et comptait son pouls .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 30: 100%|██████████| 14297/14297 [25:45<00:00,  9.25it/s, loss=1.992]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: That leather bed on which so many unhappy wretches had writhed, frightened her.\n",
      "    TARGET: Ce lit de cuir, où s’étaient tordus tant de misérables, l’épouvantait.\n",
      " PREDICTED: Cette sur laquelle tant de misérables misérables s ’ étaient , l ’ effraya .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: But the Marquise made him sit facing herself, talked to him continuously, and prevented his saying a word to her daughter.\n",
      "    TARGET: Mais la marquise le fit placer vis-à-vis d’elle, lui parla constamment et empêcha qu’il ne pût dire un mot à sa fille.\n",
      " PREDICTED: Mais la marquise le fit asseoir en face , lui parla continuellement et l ’ empêcha de dire un mot à sa fille .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 39:  30%|███       | 4327/14297 [08:01<17:47,  9.34it/s, loss=1.784]  IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 39:  49%|████▉     | 6970/14297 [12:44<13:01,  9.38it/s, loss=1.934]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 39:  68%|██████▊   | 9713/14297 [17:38<08:08,  9.39it/s, loss=1.931]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 39:  87%|████████▋ | 12401/14297 [22:25<03:22,  9.38it/s, loss=2.056]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 39: 100%|██████████| 14297/14297 [25:48<00:00,  9.23it/s, loss=2.057]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"So it's an issue of . . . ?\"\n",
      "    TARGET: -- Alors, il s'agit de... ?\n",
      " PREDICTED: -- Il s ' agit donc ... ?\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: It was eleven o'clock, and if Captain Nemo found conditions favorable for taking his sights, I wanted to be present at the operation.\n",
      "    TARGET: Il était onze heures, et si le capitaine Nemo se trouvait dans des conditions favorables pour observer, je voulais être présent à son opération.\n",
      " PREDICTED: Il était onze heures , et si le capitaine Nemo trouvait les conditions favorables à faire connaître , je désirais assister à l ' opération .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 40:   6%|▌         | 839/14297 [01:29<23:53,  9.39it/s, loss=1.804]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 40:  25%|██▌       | 3627/14297 [06:26<18:51,  9.43it/s, loss=1.934]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 40:  45%|████▍     | 6371/14297 [11:19<14:03,  9.39it/s, loss=1.761]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 40:  64%|██████▍   | 9122/14297 [16:13<09:08,  9.43it/s, loss=1.883]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 40:  83%|████████▎ | 11932/14297 [21:13<04:12,  9.36it/s, loss=1.897]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 40: 100%|██████████| 14297/14297 [25:26<00:00,  9.37it/s, loss=1.883]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"By Jove! they go in for more than that,\" exclaimed the druggist.\n",
      "    TARGET: -- Parbleu! ils en font bien d’autres! exclama l’apothicaire.\n",
      " PREDICTED: -- Eh ! ils y vont plus qu ’ ils n ’ en sont pas , s ’ écria l ’ apothicaire .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: The old gentleman was fond of money, and anxious to keep the family estate together.\n",
      "    TARGET: Le vieux M. Rochester et M. Rowland s'entendirent, et, afin d'enrichir M. Édouard, ils l'entraînèrent dans une position douloureuse.\n",
      " PREDICTED: Le vieux monsieur aimait beaucoup d ’ argent , et désirait garder les domaines ensemble de famille .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 41:   3%|▎         | 488/14297 [00:52<24:36,  9.35it/s, loss=1.706]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 41:  23%|██▎       | 3261/14297 [05:47<19:31,  9.42it/s, loss=1.977]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 41:  43%|████▎     | 6079/14297 [10:48<14:36,  9.38it/s, loss=1.950]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 41:  67%|██████▋   | 9627/14297 [17:07<08:19,  9.36it/s, loss=1.912]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 41:  87%|████████▋ | 12427/14297 [22:07<03:18,  9.41it/s, loss=1.874]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 41: 100%|██████████| 14297/14297 [25:27<00:00,  9.36it/s, loss=1.959]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Now, Cartwright, there are the names of twenty-three hotels here, all in the immediate neighbourhood of Charing Cross.\n",
      "    TARGET: À présent, Cartwright, voici les noms de vingt-trois hôtels, tous dans les environs immédiats de Charing Cross.\n",
      " PREDICTED: Cartwright , y a - t - il dans ce voisinage vingt - trois hôtels , tout est ici dans la région immédiate de Charing - Cross .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: They all offered themselves, throwing coarse chaff at her.\n",
      "    TARGET: Tous s'offrirent, la chaufferent de gros mots.\n",
      " PREDICTED: Tous s ’ offrirent , en lui jetant des plaisanteries nombreuses .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 42:   7%|▋         | 979/14297 [01:44<23:44,  9.35it/s, loss=1.812]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 42:  27%|██▋       | 3824/14297 [06:48<18:43,  9.32it/s, loss=1.735]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 42:  50%|█████     | 7154/14297 [12:44<12:43,  9.35it/s, loss=1.902]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 42:  73%|███████▎  | 10462/14297 [18:37<06:49,  9.35it/s, loss=1.926]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 42:  94%|█████████▎| 13383/14297 [23:50<01:37,  9.38it/s, loss=1.921]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 43:  14%|█▎        | 1953/14297 [03:28<21:56,  9.38it/s, loss=1.747]]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 43:  34%|███▍      | 4883/14297 [08:42<16:43,  9.38it/s, loss=1.871]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 43:  67%|██████▋   | 9531/14297 [16:59<08:28,  9.36it/s, loss=1.921]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 43:  87%|████████▋ | 12479/14297 [22:13<03:13,  9.38it/s, loss=1.676]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 44:  19%|█▊        | 2668/14297 [04:45<20:39,  9.38it/s, loss=1.732]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 44:  48%|████▊     | 6834/14297 [12:09<13:16,  9.37it/s, loss=1.776]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 44:  88%|████████▊ | 12647/14297 [22:31<02:55,  9.39it/s, loss=1.804]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 45:  29%|██▉       | 4127/14297 [07:20<18:01,  9.41it/s, loss=1.867]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 45:  79%|███████▊  | 11228/14297 [19:59<05:26,  9.40it/s, loss=2.102]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 46:  33%|███▎      | 4726/14297 [08:25<16:58,  9.39it/s, loss=1.757]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 46:  86%|████████▌ | 12288/14297 [21:53<03:35,  9.34it/s, loss=1.866]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 47:  25%|██▌       | 3596/14297 [06:24<18:59,  9.39it/s, loss=1.815]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 47:  49%|████▉     | 7032/14297 [12:31<12:54,  9.38it/s, loss=1.706]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 47:  73%|███████▎  | 10449/14297 [18:37<06:51,  9.35it/s, loss=1.798]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 47:  97%|█████████▋| 13859/14297 [24:41<00:46,  9.40it/s, loss=1.789]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 48:  23%|██▎       | 3304/14297 [05:52<19:32,  9.38it/s, loss=1.753]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 48:  47%|████▋     | 6732/14297 [11:59<13:25,  9.39it/s, loss=1.832]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 48:  71%|███████   | 10137/14297 [18:03<07:23,  9.38it/s, loss=1.804]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 48:  95%|█████████▌| 13620/14297 [24:16<01:12,  9.38it/s, loss=1.717]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 49:  31%|███▏      | 4474/14297 [07:58<17:27,  9.38it/s, loss=1.811]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 49:  56%|█████▌    | 8003/14297 [14:15<11:12,  9.36it/s, loss=1.735]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 49:  87%|████████▋ | 12463/14297 [22:12<03:15,  9.36it/s, loss=1.907]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 49:  97%|█████████▋| 13836/14297 [24:39<00:49,  9.37it/s, loss=1.762]"
     ]
    }
   ],
   "source": [
    "# from model import build_transformer\n",
    "# from dataset import BilingualDataset, causal_mask\n",
    "# from config import get_config, get_weights_file_path, latest_weights_file_path\n",
    "\n",
    "import torchtext.datasets as datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy\n",
    "\n",
    "# Huggingface datasets and tokenizers\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "import torchmetrics\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    # Precompute the encoder output and reuse it for every step\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    # Initialize the decoder input with the sos token\n",
    "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        # build mask for target\n",
    "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "        # calculate output\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "        # get next token\n",
    "        prob = model.project(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat(\n",
    "            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
    "        )\n",
    "\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0)\n",
    "\n",
    "\n",
    "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_step, writer, num_examples=2):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "\n",
    "    source_texts = []\n",
    "    expected = []\n",
    "    predicted = []\n",
    "\n",
    "    try:\n",
    "        # get the console window width\n",
    "        with os.popen('stty size', 'r') as console:\n",
    "            _, console_width = console.read().split()\n",
    "            console_width = int(console_width)\n",
    "    except:\n",
    "        # If we can't get the console width, use 80 as default\n",
    "        console_width = 80\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_ds:\n",
    "            count += 1\n",
    "            encoder_input = batch[\"encoder_input\"].to(device) # (b, seq_len)\n",
    "            encoder_mask = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)\n",
    "\n",
    "            # check that the batch size is 1\n",
    "            assert encoder_input.size(\n",
    "                0) == 1, \"Batch size must be 1 for validation\"\n",
    "\n",
    "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "            source_text = batch[\"src_text\"][0]\n",
    "            target_text = batch[\"tgt_text\"][0]\n",
    "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
    "\n",
    "            source_texts.append(source_text)\n",
    "            expected.append(target_text)\n",
    "            predicted.append(model_out_text)\n",
    "\n",
    "            # Print the source, target and model output\n",
    "            print_msg('-'*console_width)\n",
    "            print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n",
    "            print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n",
    "            print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n",
    "\n",
    "            if count == num_examples:\n",
    "                print_msg('-'*console_width)\n",
    "                break\n",
    "\n",
    "    if writer:\n",
    "        # Evaluate the character error rate\n",
    "        # Compute the char error rate\n",
    "        metric = torchmetrics.CharErrorRate()\n",
    "        cer = metric(predicted, expected)\n",
    "        writer.add_scalar('validation cer', cer, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        # Compute the word error rate\n",
    "        metric = torchmetrics.WordErrorRate()\n",
    "        wer = metric(predicted, expected)\n",
    "        writer.add_scalar('validation wer', wer, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        # Compute the BLEU metric\n",
    "        metric = torchmetrics.BLEUScore()\n",
    "        bleu = metric(predicted, expected)\n",
    "        writer.add_scalar('validation BLEU', bleu, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "def get_all_sentences(ds, lang):\n",
    "    for item in ds:\n",
    "        yield item['translation'][lang]\n",
    "\n",
    "def get_or_build_tokenizer(config, ds, lang):\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer\n",
    "\n",
    "def get_ds(config):\n",
    "    # It only has the train split, so we divide it overselves\n",
    "    ds_raw = load_dataset(f\"{config['datasource']}\", f\"{config['lang_src']}-{config['lang_tgt']}\", split='train')\n",
    "\n",
    "    # Build tokenizers\n",
    "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])\n",
    "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
    "\n",
    "    # Keep 90% for training, 10% for validation\n",
    "    train_ds_size = int(0.9 * len(ds_raw))\n",
    "    val_ds_size = len(ds_raw) - train_ds_size\n",
    "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
    "\n",
    "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "\n",
    "    # Find the maximum length of each sentence in the source and target sentence\n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "\n",
    "    for item in ds_raw:\n",
    "        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
    "        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n",
    "        max_len_src = max(max_len_src, len(src_ids))\n",
    "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "\n",
    "    print(f'Max length of source sentence: {max_len_src}')\n",
    "    print(f'Max length of target sentence: {max_len_tgt}')\n",
    "\n",
    "\n",
    "    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
    "\n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n",
    "\n",
    "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
    "    model = build_transformer(vocab_src_len, vocab_tgt_len, config[\"seq_len\"], config['seq_len'], d_model=config['d_model'])\n",
    "    return model\n",
    "\n",
    "def train_model(config):\n",
    "    # Define the device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(\"Using device:\", device)\n",
    "    if (device == 'cuda'):\n",
    "        print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
    "        print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
    "    elif (device == 'mps'):\n",
    "        print(f\"Device name: <mps>\")\n",
    "    else:\n",
    "        print(\"NOTE: If you have a GPU, consider using it for training.\")\n",
    "        print(\"      On a Windows machine with NVidia GPU, check this video: https://www.youtube.com/watch?v=GMSjDTU8Zlc\")\n",
    "        print(\"      On a Mac machine, run: pip3 install --pre torch torchvision torchaudio torchtext --index-url https://download.pytorch.org/whl/nightly/cpu\")\n",
    "    device = torch.device(device)\n",
    "\n",
    "    # Make sure the weights folder exists\n",
    "    Path(f\"{config['datasource']}_{config['model_folder']}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "    # Tensorboard\n",
    "    writer = SummaryWriter(config['experiment_name'])\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
    "\n",
    "    # If the user specified a model to preload before training, load it\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "    preload = config['preload']\n",
    "    model_filename = latest_weights_file_path(config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None\n",
    "    if model_filename:\n",
    "        print(f'Preloading model {model_filename}')\n",
    "        state = torch.load(model_filename)\n",
    "        model.load_state_dict(state['model_state_dict'])\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        global_step = state['global_step']\n",
    "    else:\n",
    "        print('No model to preload, starting from scratch')\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
    "\n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "        torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
    "        for batch in batch_iterator:\n",
    "\n",
    "            encoder_input = batch['encoder_input'].to(device) # (b, seq_len)\n",
    "            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n",
    "            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
    "            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n",
    "\n",
    "            # Run the tensors through the encoder, decoder and the projection layer\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n",
    "            proj_output = model.project(decoder_output) # (B, seq_len, vocab_size)\n",
    "\n",
    "            # Compare the output with the label\n",
    "            label = batch['label'].to(device) # (B, seq_len)\n",
    "\n",
    "            # Compute the loss using a simple cross entropy\n",
    "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            # Log the loss\n",
    "            writer.add_scalar('train loss', loss.item(), global_step)\n",
    "            writer.flush()\n",
    "\n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        # Run validation at the end of every epoch\n",
    "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
    "\n",
    "        # Save the model at the end of every epoch\n",
    "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'global_step': global_step\n",
    "        }, model_filename)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    config = get_config()\n",
    "    train_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQbgS4s4IwNa"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_XOxuOJWmxS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOa8APt3VE8AfYBM0kBl8/Z",
   "gpuType": "T4",
   "mount_file_id": "1HsVM1aDXhoYvlUBzpRwUVjLmegixDIJo",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "029b589847fc40e185dd5be4090ebe55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7f1bbd51d0fd4fabb0a8b4da01badce1",
      "placeholder": "​",
      "style": "IPY_MODEL_96f57b87f09143618274d311b06b4a0a",
      "value": "Downloading data files: 100%"
     }
    },
    "02e48a31ac7b4d29a50f491b2b31ea44": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "031e60d11dc749d7af95a349a81b72fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "03e702becc2d46f28068f230f1670923": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3d03a0dc7f41415bb70c2dc4c32f2edc",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2e5d375ba6ea46e8a997c4fd617730cd",
      "value": 1
     }
    },
    "0870ff2092df4e4990a86007ac1ed9b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0e815bd727b7497f81ae2295133c4e46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af66a79b762344cb8f3aed1fb32d61ea",
      "placeholder": "​",
      "style": "IPY_MODEL_d769fe7ac4d94869acfe809dd0f8a7c0",
      "value": "Downloading readme: 100%"
     }
    },
    "11d2382851074f8489a61e4871ec711d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_942bc81cd7874722bcf7c878f5edef8a",
      "placeholder": "​",
      "style": "IPY_MODEL_c65882c78fd245998b84f37ff2ee12b1",
      "value": "Extracting data files: 100%"
     }
    },
    "126dd435d6b044e7909dc97c1f536cc6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3df8daa0bdc14633b21a985555fbe5ed",
      "placeholder": "​",
      "style": "IPY_MODEL_ccc96502294949ccb4e3779a42b0ea26",
      "value": " 5.73M/5.73M [00:00&lt;00:00, 10.9MB/s]"
     }
    },
    "14d7f6a81ee74ab1a9815ccf4bb1bc74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_11d2382851074f8489a61e4871ec711d",
       "IPY_MODEL_03e702becc2d46f28068f230f1670923",
       "IPY_MODEL_f557bbe2d0d74c9db667d7057007a8b7"
      ],
      "layout": "IPY_MODEL_1fb272bcdc3f4962b529a93db8a0eb40"
     }
    },
    "183c1350f5e74202b856daf48238ab9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c0670ce8bcc470d967add0fc1bf8f44",
      "placeholder": "​",
      "style": "IPY_MODEL_e3ecddfae3bf48b9a3506eb553cc927d",
      "value": "Generating train split: 100%"
     }
    },
    "194bf931fd924396a8a7411c40dcd894": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1fb272bcdc3f4962b529a93db8a0eb40": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25938b5e07a642eea1c2435cf0ea8080": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2abf772dcdc042aabb2bf5b25b46ade6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2e5d375ba6ea46e8a997c4fd617730cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "311f821f5ff4474897e788dcb8b9a410": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d03a0dc7f41415bb70c2dc4c32f2edc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3df8daa0bdc14633b21a985555fbe5ed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a566846f6b04c01a8925bcf387e893b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_183c1350f5e74202b856daf48238ab9a",
       "IPY_MODEL_715f7d49656a4392b5c00766166f0125",
       "IPY_MODEL_a3bd82c96af0409eb7860b39c956b9db"
      ],
      "layout": "IPY_MODEL_194bf931fd924396a8a7411c40dcd894"
     }
    },
    "715f7d49656a4392b5c00766166f0125": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_031e60d11dc749d7af95a349a81b72fa",
      "max": 32332,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7b9a7eee6fe2479e9cbca1a79ae6643e",
      "value": 32332
     }
    },
    "7b9a7eee6fe2479e9cbca1a79ae6643e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7f1bbd51d0fd4fabb0a8b4da01badce1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f41c6b6c92e4b39af0217d3e8a82057": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a34586a33b7c48a88667e55f399d4f56",
      "placeholder": "​",
      "style": "IPY_MODEL_dc1db77b38af42879636c08865e9d2a0",
      "value": " 1/1 [00:00&lt;00:00,  1.82it/s]"
     }
    },
    "88ebeb75e06447988192f2c99c0d216a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8c0670ce8bcc470d967add0fc1bf8f44": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "92e7a01811e940c388d69aec398d836f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "942bc81cd7874722bcf7c878f5edef8a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "96f57b87f09143618274d311b06b4a0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a34586a33b7c48a88667e55f399d4f56": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a3bd82c96af0409eb7860b39c956b9db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae9442fe6f354470b59b59e4473e523f",
      "placeholder": "​",
      "style": "IPY_MODEL_88ebeb75e06447988192f2c99c0d216a",
      "value": " 32332/32332 [00:00&lt;00:00, 93170.52 examples/s]"
     }
    },
    "a60d141b29ec45dc8025c92d2991c0a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6e45638b7444eb99b584147171985e6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab9e0bfe176641ebb1ba7b2dedc9484a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_02e48a31ac7b4d29a50f491b2b31ea44",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f2e53a2f2e7a4b288b7a00f814e8d559",
      "value": 1
     }
    },
    "ae9442fe6f354470b59b59e4473e523f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af66a79b762344cb8f3aed1fb32d61ea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b6b86ec690d748d8ad7d5d99cf553527": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bfbc661de29343f89b13ccaaf119ff32": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c360499999c14412b4f4034d339da90e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f2eede091e1747c9870494e6e338ee54",
      "max": 28064,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_92e7a01811e940c388d69aec398d836f",
      "value": 28064
     }
    },
    "c6387916e7ac4810a770f58d774d7abb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c65882c78fd245998b84f37ff2ee12b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ccc96502294949ccb4e3779a42b0ea26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d4c72c83250248a181ee990c24600eca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0e815bd727b7497f81ae2295133c4e46",
       "IPY_MODEL_c360499999c14412b4f4034d339da90e",
       "IPY_MODEL_df95cdd2ba6342e69eb0fa751e95428e"
      ],
      "layout": "IPY_MODEL_bfbc661de29343f89b13ccaaf119ff32"
     }
    },
    "d769fe7ac4d94869acfe809dd0f8a7c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d7cc4632320b40d4b3d188d486095709": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc1db77b38af42879636c08865e9d2a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dea5b7677e6248a0bfa8350086ef072e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_029b589847fc40e185dd5be4090ebe55",
       "IPY_MODEL_ab9e0bfe176641ebb1ba7b2dedc9484a",
       "IPY_MODEL_7f41c6b6c92e4b39af0217d3e8a82057"
      ],
      "layout": "IPY_MODEL_a6e45638b7444eb99b584147171985e6"
     }
    },
    "df95cdd2ba6342e69eb0fa751e95428e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a60d141b29ec45dc8025c92d2991c0a7",
      "placeholder": "​",
      "style": "IPY_MODEL_c6387916e7ac4810a770f58d774d7abb",
      "value": " 28.1k/28.1k [00:00&lt;00:00, 2.76MB/s]"
     }
    },
    "e05889af986c4fd79b3fa22cac87192b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b6b86ec690d748d8ad7d5d99cf553527",
      "placeholder": "​",
      "style": "IPY_MODEL_2abf772dcdc042aabb2bf5b25b46ade6",
      "value": "Downloading data: 100%"
     }
    },
    "e3ecddfae3bf48b9a3506eb553cc927d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "edeb661a95084caf9aba4e4f24aaf302": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e05889af986c4fd79b3fa22cac87192b",
       "IPY_MODEL_ef3b575c458d4c05b26c96538713c25d",
       "IPY_MODEL_126dd435d6b044e7909dc97c1f536cc6"
      ],
      "layout": "IPY_MODEL_d7cc4632320b40d4b3d188d486095709"
     }
    },
    "ef3b575c458d4c05b26c96538713c25d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_311f821f5ff4474897e788dcb8b9a410",
      "max": 5726189,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_25938b5e07a642eea1c2435cf0ea8080",
      "value": 5726189
     }
    },
    "f21e116b03694620ad28b08e742a2b72": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f2e53a2f2e7a4b288b7a00f814e8d559": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f2eede091e1747c9870494e6e338ee54": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f557bbe2d0d74c9db667d7057007a8b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0870ff2092df4e4990a86007ac1ed9b0",
      "placeholder": "​",
      "style": "IPY_MODEL_f21e116b03694620ad28b08e742a2b72",
      "value": " 1/1 [00:00&lt;00:00, 75.49it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
